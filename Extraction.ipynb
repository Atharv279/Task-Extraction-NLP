{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           Task   Person    Action  Deadline  \\\n",
      "0  John needs to complete the report by Monday.     John     needs    monday   \n",
      "1     Sarah should schedule a meeting tomorrow.    Sarah  schedule  tomorrow   \n",
      "2    Email the client about the project update.  Unknown   Unknown      None   \n",
      "3           Finalize the budget plan this week.  Unknown  Finalize      week   \n",
      "4            Tom to review the design document.      Tom    review      None   \n",
      "\n",
      "     Category  \n",
      "0  Category 0  \n",
      "1  Category 1  \n",
      "2  Category 2  \n",
      "3  Category 1  \n",
      "4  Category 1  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag, ne_chunk\n",
    "from datetime import datetime\n",
    "\n",
    "# Download necessary NLTK data files\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "# Sample text with potential tasks\n",
    "text = \"\"\"John needs to complete the report by Monday. \n",
    "Sarah should schedule a meeting tomorrow. \n",
    "Email the client about the project update. \n",
    "Finalize the budget plan this week. \n",
    "Tom to review the design document. \"\"\"\n",
    "\n",
    "# Step 1: Preprocessing\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)  # Remove punctuation\n",
    "    words = word_tokenize(text)\n",
    "    words = [word for word in words if word not in stopwords.words('english')]  # Remove stopwords\n",
    "    return words\n",
    "\n",
    "# Step 2: Task Identification\n",
    "def identify_tasks(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    tasks = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence)\n",
    "        pos_tags = pos_tag(words)\n",
    "        \n",
    "        # Heuristic: Check if the sentence contains an imperative verb or structured task\n",
    "        contains_verb = any(tag.startswith(\"VB\") for _, tag in pos_tags)\n",
    "        contains_person = any(tag == \"NNP\" for _, tag in pos_tags)\n",
    "        contains_deadline = any(word in sentence.lower() for word in [\"today\", \"tomorrow\", \"monday\", \"week\"])\n",
    "        \n",
    "        if contains_verb or contains_person or contains_deadline:\n",
    "            tasks.append(sentence)\n",
    "    \n",
    "    return tasks\n",
    "\n",
    "# Step 3: Extract Key Entities (Person, Action, Deadline)\n",
    "def extract_entities(sentence):\n",
    "    words = word_tokenize(sentence)\n",
    "    pos_tags = pos_tag(words)\n",
    "    person = None\n",
    "    action = None\n",
    "    deadline = None\n",
    "    \n",
    "    # Extract Person\n",
    "    named_entities = ne_chunk(pos_tags)\n",
    "    for chunk in named_entities:\n",
    "        if hasattr(chunk, 'label') and chunk.label() == 'PERSON':\n",
    "            person = ' '.join(c[0] for c in chunk)\n",
    "            break\n",
    "    \n",
    "    # Extract Action\n",
    "    for word, tag in pos_tags:\n",
    "        if tag.startswith(\"VB\"):  # Verb\n",
    "            action = word\n",
    "            break\n",
    "    \n",
    "    # Extract Deadline\n",
    "    deadlines = [\"today\", \"tomorrow\", \"monday\", \"week\"]\n",
    "    for word in words:\n",
    "        if word.lower() in deadlines:\n",
    "            deadline = word.lower()\n",
    "            break\n",
    "    \n",
    "    return person, action, deadline\n",
    "\n",
    "# Step 4: Categorization with Word2Vec and LDA\n",
    "sentences = [preprocess_text(sent) for sent in identify_tasks(text)]\n",
    "model = Word2Vec(sentences, vector_size=50, window=3, min_count=1, workers=4)\n",
    "\n",
    "# Prepare LDA model\n",
    "dictionary = Dictionary(sentences)\n",
    "corpus = [dictionary.doc2bow(sent) for sent in sentences]\n",
    "lda = LdaModel(corpus, num_topics=3, id2word=dictionary, passes=10)\n",
    "\n",
    "def categorize_task(sentence):\n",
    "    bow = dictionary.doc2bow(preprocess_text(sentence))\n",
    "    topic = max(lda[bow], key=lambda x: x[1])[0]  # Get highest probability topic\n",
    "    return f\"Category {topic}\"\n",
    "\n",
    "# Step 5: Generate Structured Output\n",
    "structured_tasks = []\n",
    "for task in identify_tasks(text):\n",
    "    person, action, deadline = extract_entities(task)\n",
    "    category = categorize_task(task)\n",
    "    structured_tasks.append({\n",
    "        \"Task\": task,\n",
    "        \"Person\": person if person else \"Unknown\",\n",
    "        \"Action\": action if action else \"Unknown\",\n",
    "        \"Deadline\": deadline if deadline else \"None\",\n",
    "        \"Category\": category\n",
    "    })\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "df = pd.DataFrame(structured_tasks)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenges Faced\n",
    "\n",
    "#### - Identifying Proper Names: Extracting responsible persons required handling capitalization and POS tagging carefully.\n",
    "\n",
    "#### - Task Detection Heuristics: Simple keyword-based matching sometimes led to false positives.\n",
    "\n",
    "#### - Categorization Accuracy: Without pre-trained embeddings, task categorization required fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insights Gained\n",
    "#### - Preprocessing Matters: Removing stopwords and careful tokenization significantly improved entity extraction.\n",
    "\n",
    "#### - POS Tagging is Crucial: Using POS tags helped refine task detection and person identification.\n",
    "\n",
    "#### - Iterative Refinement: Debugging outputs for different inputs helped refine heuristics for better accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
